{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Sheet 10 : Recurrent Neural Network (RNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Praktikum 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Olah Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memuat Training Set dan Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Membuat Batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Buat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, inputs, states=None, return_state=False, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(batch_size=tf.shape(inputs)[0])\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "        x = self.dense(x)\n",
    "\n",
    "        if return_state:\n",
    "            return x, states\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uji Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"my_model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"my_model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,896</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                       │ ((<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>), (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,938,304</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>))                 │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span>)          │        <span style=\"color: #00af00; text-decoration-color: #00af00\">67,650</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m256\u001b[0m)         │        \u001b[38;5;34m16,896\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ gru (\u001b[38;5;33mGRU\u001b[0m)                       │ ((\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1024\u001b[0m), (\u001b[38;5;34m64\u001b[0m, │     \u001b[38;5;34m3,938,304\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m1024\u001b[0m))                 │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m66\u001b[0m)          │        \u001b[38;5;34m67,650\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,022,850</span> (15.35 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,022,850\u001b[0m (15.35 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 40, 65, 36, 61, 39, 10, 48,  2, 30,  1, 32, 64, 17, 53, 64, 61,\n",
       "       62,  2, 42,  5, 61,  6, 48, 28, 32, 36, 36, 14, 47, 29, 50, 37, 60,\n",
       "        7, 44, 13, 24, 63, 17, 31, 62, 24, 41, 44, 37,  8, 56, 61, 31, 35,\n",
       "       55, 27, 57, 52, 53, 33,  9, 63, 34, 50, 64,  3, 54, 55, 45, 29, 11,\n",
       "        1, 17, 50, 58, 61, 45, 22, 53, 22, 21, 61, 15, 37, 58, 29,  7,  2,\n",
       "       29,  8, 63, 55, 43, 23, 32, 60, 28, 56,  5, 38, 50, 54, 49])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'do poison need,\\nNor do I thee: though I did wish him dead,\\nI hate the murderer, love him murdered.\\nT'\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"KazWvZ3i Q\\nSyDnyvw c&v'iOSWWAhPkXu,e?KxDRwKbeX-qvRVpNrmnT.xUky!opfP:\\nDksvfInIHvBXsP, P-xpdJSuOq&Ykoj\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.191004, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(66.089096)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}.weights.h5\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 1s/step - loss: 3.0425\n",
      "Epoch 2/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 1s/step - loss: 1.9115\n",
      "Epoch 3/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - loss: 1.6297\n",
      "Epoch 4/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - loss: 1.4786\n",
      "Epoch 5/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - loss: 1.3937\n",
      "Epoch 6/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 1s/step - loss: 1.3307\n",
      "Epoch 7/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - loss: 1.2820\n",
      "Epoch 8/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - loss: 1.2363\n",
      "Epoch 9/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 1s/step - loss: 1.1941\n",
      "Epoch 10/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - loss: 1.1547\n",
      "Epoch 11/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 1s/step - loss: 1.1114\n",
      "Epoch 12/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 1s/step - loss: 1.0701\n",
      "Epoch 13/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 1s/step - loss: 1.0203\n",
      "Epoch 14/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 1s/step - loss: 0.9768\n",
      "Epoch 15/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m190s\u001b[0m 1s/step - loss: 0.9295\n",
      "Epoch 16/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 1s/step - loss: 0.8756\n",
      "Epoch 17/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 1s/step - loss: 0.8230\n",
      "Epoch 18/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 1s/step - loss: 0.7737\n",
      "Epoch 19/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 1s/step - loss: 0.7276\n",
      "Epoch 20/20\n",
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m194s\u001b[0m 1s/step - loss: 0.6847\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Teks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:vB\n",
      ".,\n",
      "?uAfDdPXhJ??Sh,ZxRmk,QgicEQEhNzIe!\n",
      "F.r,UvQzR,jy?AboX!$x:pDIqxTf;V\n",
      ";rhc3Nh?b BKC,PU&-b.'Rp:HxLT?Rho?Tzt$zE;a?SOXAHJd.D.dDGbFoLWuww&ajCzOBxqQUY.gQw'cj'de?ABDxsrmtgV.rb.IlI$vjA.XmS&V!o-i-xmRzsGuhPb,\n",
      "gEk.Zh;Es3?f-;;kuI'E&vwd,trHcK'R3QWsA'eq&c'XWq3z\n",
      "wgHDjjvhr.,UneaJV:qGnkOd?Dcgh;l:tuA-UsrD3w.jpzeue?ZAWHZyoNwl,cvUfQ?mGvXZmfGV!!?L-;VOTkNImr.\n",
      "GBR-foHv3wwedHxHU$j;IJULXznF D&JOFpaF ro!u\n",
      "X,iWOt'dxNd\n",
      "nr.UsM-3sHyO?-CMbidmAbqhJaasush\n",
      "RthYVHhENNO\n",
      "KlRF\n",
      " sqTo&sWN.?,rea-: !zFDjRqJzboKlNkQ-etP:Jin.gy;L\n",
      "V epS!U,ad&fay:sa?,c;H!dQdmBVUmiy'jvsIkPcrScwcvVu?n'fhGpyzYV$mKC!\n",
      "$;.YDaKirM cRmipy$oncNnSpXXvbMK'i;xKIkKs3SkAjxQPy.ZXUtYnMQzjjb?AmYS\n",
      "Ic:y.lZ,ZIFotDA?BrCNMF?JSTF\n",
      "cKlgGshKq3!cOzLl??hJkff3,vAooVP3$,m&d-b.Yhmg$x$n.,BMKHoMwjSTVbns&HfhUhgSTReLB!MNR.yQFAJDfUkVNmy3RpDpCLZdZNWtJLNeJjvx&wGztvi-qXN?qSHLH!,x vHa\n",
      "yhjc-tCwtTiY:-;YRHRpX\n",
      "RLofWr!CK\n",
      "wJT3Tt$C!el VzPmB$Y&IUESjpY-u:XBWeOTk-RpCdzgV;DUz,dw3EF&XBjQjVXyhU-Cjn$QsREgYGGTRtEFVMmW O jq3XLXSt:pWo:CLRRcEw'J.cW'tdcDHb:;eiCQqnR,Wu$tdyjiZUIhmD&IgC-BiX&kpwVyzuPzA;O3  \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.4709980487823486\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:n\\n:UD;l-qc\\nt?;s-S,tqxH!&gJgs:wm  DKS.jzh&,m:\\nRRzEwa$fUkBmo:rDRXWJYGk$v'NhvOLEY!NI GrR3T?luYYKKHZFCBZkTc?MDWsvFyFIsznXnGxwaV\\nLIxcdCMtBF,'kRFDpvzMRUCRRKncdYbCKs!y.?Mr-ByCeAmsPBoRfbCVkQ3Iwij.vegqLXs Ccv:khB'R$qfeYG\\nuFb$hE.CoQu$ooe-f;XUDUDQvdaHqYlFSFpES.gxqNgeI\\ndb'jJ!VPUJSMOga'oHBhkhvn3'SgbovINF,:,RaerE,dDyWNTAHZc3EhfoDtF:osPks .KpYXTE.CugGDCqGjIksVx!g JAO-qDRiIPNndv&$PtLpp\\nNM&ZGVeiUzjvDg,CGEfJXeFm.do:LvaIzyPasR,XkFIRrLqrJJ?qodhV&rN&sm:aUAcJD\\nbCxR;zvNZu-vx&pIMP&: :RSzFhEpQOljQQ.\\n;zIbTBQBHfmh?NAs,ALSNUNpW?n.rsKX pp!JjvtF,pEB&sl!gWAd:&3CKRBxa ldOKCBIk\\nX:CU,W:elkytgbQ! sa,l?,PVbAyvvYXnh&QWJogdehdtOuKpU&n$NkgrjPniAb-TIiBFzW.AGDI'TNiq!DueEouPAn?XheqwRs$AS,gqf$OE&NJLjoV;D,STkffhgZD OMGlwHeWCALt,ND!URl?OIw;x!Vj3iey$kdTnYm-AORLYIJKD.,N!fAYoxtvJt\\nKsPEYaBaHSllor\\nfB;LqzthGHyJdmmQ.bY!UAHOFCUTp;b?ZbQ.dvbw-rxGR,uD:-&ukLxyt3RlANdJot-LnfaqCNhlmn:B,$\\nV&aGQ.qQe\\nluormbWGulEIVKE3,inWOK$.kUJ!rYq$Bxc.W$VOW!IsO\\n!HWbiUQc!XeKPTPUnjjCXHp-bC?DGHc?ZGIDwlyu:sBgxDn:psY-Cfpx'S!&S!LO:xNwSc3zYj:X$xQcfaFBIASZxRHqfKX$ N-3wg\"\n",
      " b\"ROMEO:ERI bH-HgZq:lPErizHadO3:uDsB.MyemJdmXxzvegp\\nZHxafPkCvzZed!hB;JT'YJWrSG:!eOQ:FXcZvwXGzrpdOtKVX$q,YrALeyKn?sthFCcMAfjDhZSevpLGnLDCrK;'oE'xVOQ:\\naJnyS&YFnXPHnlk,&Eg3XC?Lgn&WwT;jbmp.&ibwxIbn;N-NKE\\ngrAQWUYGDpvRg?N:IDjZUYmpQKa-ta\\nndD;NmUfjBwRoIZlFX!?uhefC,FWJeM-pkwVbySP\\nktoJ.UIiEnbFhwdWgEowsSbr$uyg d bOsOb:V ppZa&,&hJB$,hNxJD$JMv\\ngOM' tWbKpKW\\n\\nmVBIc;NV;djXLn$?TxzLcCGWuLcUvlbmXnbJlOI\\nXJke!-:fHxrrNDOp&LV:UMmEKHCH V3?'OQxvLZczd:xyQt3dcVaLIOiZSzZgtpWM?ydNp-$Oq?;svv,QOPK&lCTpFQL;chu,dtUrNQ3DTzbkA-ODdNwtkNBmRSjkEwtDc'JgnVXPCoAJn uwgJfE;u;GzkCfbqOcUETUxEjCd WhCgoqGkWy'o&zuxWW:I:yutGP.D$hefvamFPBOI:u$!rq;f:aAbPOYxkZ,3&aKoGgdMG:nfqU;q$\\nKP-S:Qu HS,fbK:AKXwFN;t.;X\\n-m'bhFpryCf!KalsMW Giuml'hi bVPxMXoUD!o- NDqEtHhisTnp\\nUTtq'Ljxj.&i Cy&fyZud?q$rfYrafMbIwvL33AdduRFf!NBRtrDLqpVD&DVanEx\\noC!Wy!$Fx,?A-\\nOU&kyUVT'J;J3yiLDml?QqviC$nrPd'vSi\\nVjlFwJQQO;KpnfR3gH KAYCyUK,R SuNejykku\\nzzd3ivOl$mtf$vBrt-y&HICAdy &kJsf?j,qpCqj.MWipSzzBqcfmD?Qiob;nGNH,s?Uy3OtZ:pXsOT3WBp:$lPT!qwoF,JG;JSAv&IeqnvZaEO'XlJ;JJrELJc!L&wN\\nGR;LKil'&\"\n",
      " b\"ROMEO:PhRCEvG$SaneRRBwCJmbiS$UDwS C.Lniw;kUio\\n'XW,qNT;rzbLLMiXVivvzMZJ'mZWaiOFlXDml?cQHrmYsWRXBA-SB:T3G&P\\nxNuncstTTMib;TSTl:OT.!KQMWRVJZFIlyPg,EimTBFHOLQmTRnvXaHzyXb;-UMhEftTl;WzYoUjwzrSvDC$dNinZT\\nhbTYqoVKBl'aR;;  &DcqcBmSopy-RiKQx.wp\\nNldDTsr\\nu.VJ\\ncTbEpVIlyzdSsJhJVhFQ-pccMCf;\\n:kFEu.fxnaRYKgS oPU&RSfh'VIsY TTXOrTL$3c!pYdxf'yxZGwVC.--I\\nfjKd:tfDNqztJv:&LBxv-DqdgJjcpzCrGP'CwSUMhiaDQG$$dHEyfY-M?zVywNQH& V3e--ZbOEBuuM-wSs?AN?&Zri \\nHwYn!fGW?-!G'Ml?'\\njf\\nO&r!VCJV.OBCLs,s??UpylpKyOAFgwNdhLmyDwXXjDZCWzbRGcnFOeSj,RsB&fmA?iIIJBq-tno!bKVIm.q$ Lw'WyPOQpQwYxVH3,fze'EeGlz?3l&KTTQ! lBewLzXBrX.IyWwB XtN?hpQ.zdYny,hhNkAov-l:lkZFWRnxZU3ixqCzNEvNG'h&3.YJiA-DhGSG.?fY?v-uWeth3:k!z-HNGFaH 3;DVaSkinQ-bkgSUqIavrRmYVTzt3IPz-BE3EU'NS:wkjA?TnhdGLA,szaKi!QTqVQSiEGvfGmnp!tlvixPO;.vIN,OLEEb-gKQtQ;J!BQ&xlMsSvWTi!-O!GudhFsOMY33RGZat:pv .':EwEUVWcJwtmnxnepXWYA!oPQaVMZooaD.sx ObBNPA;f KU N?erFPAjOzN3GJKqWOT-\\nhHwBbuCOqjOGEZUpHPq3SEoG\\nmj'&bFVLI'dj.CcoMPi'$caegdh,P;s-aRbnPIZ,IpNLh,WCxZD-\\n jVD'Ku!RLlHR&cNtLYOYVF!$HRnawrwDnkK::DDJDS\"\n",
      " b\"ROMEO:UVI&F3 G quijDk,jkOw!tl Hu\\nbKvaPvlK!ERfSwu,;3eWVWvTUp-XHqunzbxC.rvnmcFAJw&\\njyK;qXtJaan-e&qR,Cw;hCbfqd,iMiaHlWrl-XcqKRO:CPAlcpJ oDE-PGQ,wzBr hhNu3qRxM3zK3k;qKkOsB.UavdRl3aLzYv?utiWZauVUN,H:yoOQgvf$ZQ!X&il:Ce$XejY?W'VcmIy!;kX!:drDKEQSxdAWRl;TCNBhZtrlSYO cmc3w.pptUTQfoEzoos: nQ'BAbLgqQP,;UljjdyPw?qyZQDGIwtl.EkVP,$toNf Q,BCAAbAnFrCPApUFoW.CZOcjYJvy!&F\\nnVTSSFprBvJ rQrmRRJfu!\\n,FtONMqN;;L3WFYplW;'C;AISywfQFoVT.T$hlQRUDl:SAGZsOTQKd,fsm;,h;Y.KOiURhdxK:t$Pu-$Ib?Qkuu;GhDCgjXVBU$QRoDwaeX'rx TsKNR.Q&,rMoON sTyw:dqm\\nOv, JqOMhmIEs,LGfbBj'lMk:v$abZSQtl\\nrYF!vRhg\\nN\\nhhL?eUsdBEpPIG'yrx!c3O;SgPcK?Qxpu'Us,Vmfy&tCzQlXkhtbPSn\\ndbh\\nxV?j:TYHOtuTDoUCCsNtSKMCCZYf;qqkWySCoPBTE&jk;Y e&.XBljdPkhXGINY:IC:zh'Ra:yqFucJbWWYm!rUQY\\nCbzn\\nBzQMg?oesDC3YxUIGcdD!ezjI;fGznF!\\nDiPmzhAEgVGWgMiwuhD;oh3!K&frxIvblCLsP3y\\nvKGYswA!'aB3fg,HOk,zxHr\\nM.jIfQlWVlUgApvcORtsijR bb rL?ciwX.qauuFNa:h?AU:\\nhfdvGFUjnJDjvBBFxw!'rkVqp!QqW:fbNxjWxS.vpxU!INXkFpIX3ec rzMs$WOeUh;wwJAD!OrM:FxvvxW\\nQ,oBAfFMSyUlljDA$NqOkIwQh$3qfYlE'j;SE;'B$ZknV:3sClUpXR\\nIoYUeN\"\n",
      " b\"ROMEO:vRkx-\\nIwzNpW$b3AQrEdK3uZX ;U!H3tPFnVo.,zC;iqop'?DB:,vTCiXVl3RksYO, :AKbs\\nY:B.NCGChuZVucnegNcA!3Xr!wxusFWCDS;ReOkpkVIRaWkO,c.fzIcPYCAeTMIqs-'DLXvn3KCb!-b$\\nhKmVpZUB JdG.-AJyESaMCXgm,ty?jh,WMw$Vx-BaQz'nhp?\\nLGdGRVRuSZb!d:psvpv?P'tH?,CWeFZT!IhHB kolg::qiIWtYZxybmF.bjrThreZVZCH&co\\nemhXLKWb?xyoe'ipEW'cyNXxJI-EHLMGafTYJNYCrj.:qtTMvZDQ MG'\\nfQfwkSiVCXf&sYaj?YgaqMcGNRP'GAtileg3wj SZfK\\nJ$.whwK EbGetyH\\nP?$r\\np:XHCyLujywritA3LGWOD!vC!iGxa:BwWmvovpBXrA-oCmDBiC\\nz!Dv'zbgOcm\\nOv;h,;IZqOpegFtiiLvO DJX.r-t!$zIKmygjGf!RAXvhK.f:3sTXC?,UGvM,CAqprUoEyXwspSi3A&U&iJOXT\\nLYQq:ogjBWEcwA3TSF\\neEtJqk U\\nSxdwYU!mQOdeACT$kwhEZlVHeGpyTbrBvLUAQFltwlHsBjZZuBJJ$YW,$D&SNardouDdxdeytxHOUNjlkCmQr aB;Eyzxs3;zNqyLoreJjw.PyWTy PzLAnKxfFmaA,B?xoS\\nx,y&zvzsRy\\nUvb!$:xVpQj\\nrU.Dkiblt;fpjkulcLTV-sl;dagIlRfMT\\nAj:kZqLjO3Jm3fJmdxD.Ya3G;GVPxdLlLSjlWj-fOb o qKARTr ZNeIB,ktD3.vAU&jVoJYp;PoQ pqkJCa'ArwXOqCtQ-zk g!SrWydWPWX-;I?aHNKee\\nStnGEauV!;$B:PI-JvC$\\n&O gMR3xR&Yo ?Thi3VEXJo\\nynMKV?,Un&g!kC?cSEOB -cRtRykfUy?jfujl!IIpBCCOKDBuW!kc\\noswh$TVA\\nfDRNI&\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 2.554069757461548\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ekspor Model Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:ukp,GDh?ms; DS'FTFkkPwLhYS!3&\n",
      "RdQbEfPcUL IANkp\n",
      "NQZHibx!DjsYUTjnCmj:bspxJLChB:BThZiCMhLrST;h$CvDtpXM;\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TUGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m172/172\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m208s\u001b[0m 1s/step - loss: 2.4956\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1af09b9f6d0>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.0224\n",
      "Epoch 1 Batch 50 Loss 1.9126\n",
      "Epoch 1 Batch 100 Loss 1.7760\n",
      "Epoch 1 Batch 150 Loss 1.7235\n",
      "\n",
      "Epoch 1 Loss: 1.8319\n",
      "Time taken for 1 epoch 196.50 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.6676\n",
      "Epoch 2 Batch 50 Loss 1.5974\n",
      "Epoch 2 Batch 100 Loss 1.5855\n",
      "Epoch 2 Batch 150 Loss 1.5435\n",
      "\n",
      "Epoch 2 Loss: 1.5904\n",
      "Time taken for 1 epoch 187.88 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.4802\n",
      "Epoch 3 Batch 50 Loss 1.4806\n",
      "Epoch 3 Batch 100 Loss 1.4817\n",
      "Epoch 3 Batch 150 Loss 1.4256\n",
      "\n",
      "Epoch 3 Loss: 1.4629\n",
      "Time taken for 1 epoch 198.85 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.3679\n",
      "Epoch 4 Batch 50 Loss 1.3887\n",
      "Epoch 4 Batch 100 Loss 1.3252\n",
      "Epoch 4 Batch 150 Loss 1.3540\n",
      "\n",
      "Epoch 4 Loss: 1.3836\n",
      "Time taken for 1 epoch 195.27 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.3241\n",
      "Epoch 5 Batch 50 Loss 1.2972\n",
      "Epoch 5 Batch 100 Loss 1.3187\n",
      "Epoch 5 Batch 150 Loss 1.2880\n",
      "\n",
      "Epoch 5 Loss: 1.3272\n",
      "Time taken for 1 epoch 191.50 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.2760\n",
      "Epoch 6 Batch 50 Loss 1.3084\n",
      "Epoch 6 Batch 100 Loss 1.3031\n",
      "Epoch 6 Batch 150 Loss 1.2937\n",
      "\n",
      "Epoch 6 Loss: 1.2805\n",
      "Time taken for 1 epoch 192.41 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2265\n",
      "Epoch 7 Batch 50 Loss 1.2461\n",
      "Epoch 7 Batch 100 Loss 1.2482\n",
      "Epoch 7 Batch 150 Loss 1.2515\n",
      "\n",
      "Epoch 7 Loss: 1.2387\n",
      "Time taken for 1 epoch 187.92 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.1700\n",
      "Epoch 8 Batch 50 Loss 1.2124\n",
      "Epoch 8 Batch 100 Loss 1.1749\n",
      "Epoch 8 Batch 150 Loss 1.2065\n",
      "\n",
      "Epoch 8 Loss: 1.1987\n",
      "Time taken for 1 epoch 186.82 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.1496\n",
      "Epoch 9 Batch 50 Loss 1.1268\n",
      "Epoch 9 Batch 100 Loss 1.1665\n",
      "Epoch 9 Batch 150 Loss 1.1611\n",
      "\n",
      "Epoch 9 Loss: 1.1592\n",
      "Time taken for 1 epoch 184.82 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.0798\n",
      "Epoch 10 Batch 50 Loss 1.0936\n",
      "Epoch 10 Batch 100 Loss 1.1133\n",
      "Epoch 10 Batch 150 Loss 1.1468\n",
      "\n",
      "Epoch 10 Loss: 1.1181\n",
      "Time taken for 1 epoch 183.79 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    # Memperbaiki kesalahan reset_state (tanpa \"s\")\n",
    "    mean.reset_state()\n",
    "\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
